{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About LBNLP \u00b6 LBNLP is a joint natural language processing (NLP) effort between the Ceder group , HackingMaterials Group , and Persson Group at Lawrence Berkeley National Laboratory natural aimed at materials science text. LBNLP is currently being migrated and condensed from several source repos, and it is currently in an experimental state. \u00b6 That being said, some of the things you can currently do with LBNLP are: Preprocess and tokenize text with methods specific to materials science \u00b6 The MatScholarProcess class implements methods specific to materials science and chemistry, including phrasing of specific materials-science lingo. Load pre-trained models in 2 lines of code \u00b6 LBNLP also gives access to open-source pre-trained NLP models such as: BiLSTM-based named entity recognition for solid state materials entities (such as characterization methods) BERT-based named entity recognition (MatBERT) for general solid state materials BERT-based named entity recognition (MatBERT) for specific downstream tasks (identifying Au nanoparticle morphologies, identifying dopants and host materials) Logistic regression/TFIDF-based relevance classification for identifying whether text is relevant specifically to solid-state materials science With the following models to come: - MatBERT for doping analysis using an improved annotation scheme for improved accuracy - Doc2Vec for suggesting journals based on input text alone - Access to Mat2Vec embeddings from the original Mat2Vec paper Each of these models is condensed to an extremely simple interface (1 method to run inference) can be loaded in 2 lines of code. See the Pretrained documentation on the side bar for more information on how to run, load, and interpret these models.","title":"About LBNLP"},{"location":"#about-lbnlp","text":"LBNLP is a joint natural language processing (NLP) effort between the Ceder group , HackingMaterials Group , and Persson Group at Lawrence Berkeley National Laboratory natural aimed at materials science text.","title":"About LBNLP"},{"location":"#lbnlp-is-currently-being-migrated-and-condensed-from-several-source-repos-and-it-is-currently-in-an-experimental-state","text":"That being said, some of the things you can currently do with LBNLP are:","title":"LBNLP is currently being migrated and condensed from several source repos, and it is currently in an experimental state."},{"location":"#preprocess-and-tokenize-text-with-methods-specific-to-materials-science","text":"The MatScholarProcess class implements methods specific to materials science and chemistry, including phrasing of specific materials-science lingo.","title":"Preprocess and tokenize text with methods specific to materials science"},{"location":"#load-pre-trained-models-in-2-lines-of-code","text":"LBNLP also gives access to open-source pre-trained NLP models such as: BiLSTM-based named entity recognition for solid state materials entities (such as characterization methods) BERT-based named entity recognition (MatBERT) for general solid state materials BERT-based named entity recognition (MatBERT) for specific downstream tasks (identifying Au nanoparticle morphologies, identifying dopants and host materials) Logistic regression/TFIDF-based relevance classification for identifying whether text is relevant specifically to solid-state materials science With the following models to come: - MatBERT for doping analysis using an improved annotation scheme for improved accuracy - Doc2Vec for suggesting journals based on input text alone - Access to Mat2Vec embeddings from the original Mat2Vec paper Each of these models is condensed to an extremely simple interface (1 method to run inference) can be loaded in 2 lines of code. See the Pretrained documentation on the side bar for more information on how to run, load, and interpret these models.","title":"Load pre-trained models in 2 lines of code"},{"location":"pretrained/01how2load/","text":"Loading pretrained models \u00b6 LBNLP provides easy access to many ready-to-use models for NLP tasks such as: Named entity recognition for inorganic materials (via LSTM) Relevance classification for materials science-related texts (via Logistic Regression) Suggestions for scientific journals (via Doc2Vec) Word2Vec embeddings for various materials-related entities And more! View available models \u00b6 View what models are currently available with print_models_info from lbnlp.models.rolodex import print_models_info print_models_info () Related models and tools are packaged together in \"model packages\". Model Package: ' matscholar_2020v1 ' * ' ner ' : Named entity recognition for materials . - More info: https: //doi.org/10.1021/acs.jcim.9b00470 * ' ner_simple ' : A simpler tagging scheme version of the regular matscholar2020v1 NER model . - More info: https: //doi.org/10.1021/acs.jcim.9b00470 * ' relevance ' : Relevance classification with logistic regression for materials science abstracts . - More info: None Model Package: ' matbert_ner_2021v1 ' * ' aunp2 ' : Named entity recognition for gold nanoparticle adjective descriptions ( DES ) and noun morphologies ( MOR ) using BERT pre - trained on materials science text . - More info: forthcoming * ' aunp11 ' : Named entity recognition for gold nanoparticle with 11 labels using BERT pre - trained on materials science text . - More info: forthcoming * ' solid_state ' : Named entity recognition for solid state materials data using BERT pre - trained on materials science text . - More info: forthcoming * ' doping ' : Named entity recognition for solid - state doping [ DOPANT ( dopant species ), BASEMAT ( host material ), DOPMODQ ( dopant quantity or carrier density )] using BERT pre - trained on materials science text . - More info: forthcoming Here, we see there are two models packaged together in the matscholar_2020v1 package and four are packaged together in matbert_ner_2021v1 . The model package names and the model names are what we will use to load them in the following code. Load a pretrained model (NER) \u00b6 Model downloads and file management are done automatically. You only need to call a single load function from the model package module in order to load and deploy the model locally. # Import the load function from the model package from lbnlp.models.load.matscholar_2020v1 import load ner_model = load ( \"ner\" ) If you get a ModelReqirementError , we recommend installing the package (with specific version) listed in the error. This is to avoid silent errors and annoying errors, as our models were generated with specific dependencies. Our NER model is now loaded. Let's see what it can do! doc = \"CoCrPt/CoCr/carbon films were sputter-deposited on CoTaZr soft-magnetic\" \\ \"underlayers and the effects of a carbon intermediate layer on magnetic and \" \\ \"recording properties were investigated\" tags = ner_model . tag_doc ( doc ) print ( tags ) The tokens in the doc are annotated: [[('CoCrPt', 'B-MAT'), ('/', 'O'), ('CoCr', 'B-MAT'), ('/', 'O'), ('carbon', 'B-MAT'), ('films', 'B-DSC'), ('were', 'O'), ('sputter', 'B-SMT'), ('-', 'I-SMT'), ('deposited', 'I-SMT'), ('on', 'O'), ('CoTaZr', 'B-MAT'), ('soft', 'B-PRO'), ('-', 'I-PRO'), ('magnetic', 'I-PRO'), ('underlayers', 'I-PRO'), ('and', 'O'), ('the', 'O'), ('effects', 'O'), ('of', 'O'), ('a', 'O'), ('carbon', 'B-MAT'), ('intermediate', 'O'), ('layer', 'B-DSC'), ('on', 'O'), ('magnetic', 'B-PRO'), ('and', 'O'), ('recording', 'B-PRO'), ('properties', 'I-PRO'), ('were', 'O'), ('investigated', 'O')]] Working with models \u00b6 Since many of the models were designed, trained, and deployed by different authors with different goals, consult the documentation for the specific model you are interested in under the \"Pretrained\" section of this documentation. Each model has its own unique methods to call to obtain the desired results, so follow the guides on the individual pages for the best results. Each model package will have it's own specific requirements! . You can find these in the root lbnlp directory as .txt files interpretable by pip , and alternatively in the model package metadata - we will do our best to warn you if a required package is not found before loading a model.","title":"Loading pretrained models"},{"location":"pretrained/01how2load/#loading-pretrained-models","text":"LBNLP provides easy access to many ready-to-use models for NLP tasks such as: Named entity recognition for inorganic materials (via LSTM) Relevance classification for materials science-related texts (via Logistic Regression) Suggestions for scientific journals (via Doc2Vec) Word2Vec embeddings for various materials-related entities And more!","title":"Loading pretrained models"},{"location":"pretrained/01how2load/#view-available-models","text":"View what models are currently available with print_models_info from lbnlp.models.rolodex import print_models_info print_models_info () Related models and tools are packaged together in \"model packages\". Model Package: ' matscholar_2020v1 ' * ' ner ' : Named entity recognition for materials . - More info: https: //doi.org/10.1021/acs.jcim.9b00470 * ' ner_simple ' : A simpler tagging scheme version of the regular matscholar2020v1 NER model . - More info: https: //doi.org/10.1021/acs.jcim.9b00470 * ' relevance ' : Relevance classification with logistic regression for materials science abstracts . - More info: None Model Package: ' matbert_ner_2021v1 ' * ' aunp2 ' : Named entity recognition for gold nanoparticle adjective descriptions ( DES ) and noun morphologies ( MOR ) using BERT pre - trained on materials science text . - More info: forthcoming * ' aunp11 ' : Named entity recognition for gold nanoparticle with 11 labels using BERT pre - trained on materials science text . - More info: forthcoming * ' solid_state ' : Named entity recognition for solid state materials data using BERT pre - trained on materials science text . - More info: forthcoming * ' doping ' : Named entity recognition for solid - state doping [ DOPANT ( dopant species ), BASEMAT ( host material ), DOPMODQ ( dopant quantity or carrier density )] using BERT pre - trained on materials science text . - More info: forthcoming Here, we see there are two models packaged together in the matscholar_2020v1 package and four are packaged together in matbert_ner_2021v1 . The model package names and the model names are what we will use to load them in the following code.","title":"View available models"},{"location":"pretrained/01how2load/#load-a-pretrained-model-ner","text":"Model downloads and file management are done automatically. You only need to call a single load function from the model package module in order to load and deploy the model locally. # Import the load function from the model package from lbnlp.models.load.matscholar_2020v1 import load ner_model = load ( \"ner\" ) If you get a ModelReqirementError , we recommend installing the package (with specific version) listed in the error. This is to avoid silent errors and annoying errors, as our models were generated with specific dependencies. Our NER model is now loaded. Let's see what it can do! doc = \"CoCrPt/CoCr/carbon films were sputter-deposited on CoTaZr soft-magnetic\" \\ \"underlayers and the effects of a carbon intermediate layer on magnetic and \" \\ \"recording properties were investigated\" tags = ner_model . tag_doc ( doc ) print ( tags ) The tokens in the doc are annotated: [[('CoCrPt', 'B-MAT'), ('/', 'O'), ('CoCr', 'B-MAT'), ('/', 'O'), ('carbon', 'B-MAT'), ('films', 'B-DSC'), ('were', 'O'), ('sputter', 'B-SMT'), ('-', 'I-SMT'), ('deposited', 'I-SMT'), ('on', 'O'), ('CoTaZr', 'B-MAT'), ('soft', 'B-PRO'), ('-', 'I-PRO'), ('magnetic', 'I-PRO'), ('underlayers', 'I-PRO'), ('and', 'O'), ('the', 'O'), ('effects', 'O'), ('of', 'O'), ('a', 'O'), ('carbon', 'B-MAT'), ('intermediate', 'O'), ('layer', 'B-DSC'), ('on', 'O'), ('magnetic', 'B-PRO'), ('and', 'O'), ('recording', 'B-PRO'), ('properties', 'I-PRO'), ('were', 'O'), ('investigated', 'O')]]","title":"Load a pretrained model (NER)"},{"location":"pretrained/01how2load/#working-with-models","text":"Since many of the models were designed, trained, and deployed by different authors with different goals, consult the documentation for the specific model you are interested in under the \"Pretrained\" section of this documentation. Each model has its own unique methods to call to obtain the desired results, so follow the guides on the individual pages for the best results. Each model package will have it's own specific requirements! . You can find these in the root lbnlp directory as .txt files interpretable by pip , and alternatively in the model package metadata - we will do our best to warn you if a required package is not found before loading a model.","title":"Working with models"},{"location":"pretrained/02ner/","text":"Models - Named Entity Recognition \u00b6 BiLSTM-NER for Solid State Materials Data \u00b6 matscholar_2020v1 - ner \u00b6 The BiLSTM-NER model tags inorganic solid-state entities in materials science. Read more details in the original publication. We can import this model from the matscholar_2020v1 model package. Note: this model requires a specific, older version (1.15.0) of tensorflow in order to run. # Import the load function from the model package from lbnlp.models.load.matscholar_2020v1 import load ner_model = load ( \"ner\" ) Our NER model is now loaded. Let's annotate a document. doc = \"CoCrPt/CoCr/carbon films were sputter-deposited on CoTaZr soft-magnetic\" \"underlayers and the effects of a carbon intermediate layer on magnetic and \" \"recording properties were investigated\" tags = ner_model . tag_doc ( doc ) print ( tags ) The tokens in the doc are annotated as tuples according to the IOB (inside entity I , outside entity O , beginning of entity B ) scheme to handle multi-token entities and the following entity tags MAT : material DSC : description of sample SPL : symmetry or phase label SMT : synthesis method CMT : characterization method PRO : property - may also include PVL (property value) or PUT (property unit) APL : application An example tag would be B-MAT meaning beginning of a material entity followed by I-MAT meaning a continuation of that material entity. The output of our code is a list of sentences - each sentence is a list of tuples coupling the text with the label: [[('CoCrPt', 'B-MAT'), ('/', 'O'), ('CoCr', 'B-MAT'), ('/', 'O'), ('carbon', 'B-MAT'), ('films', 'B-DSC'), ('were', 'O'), ('sputter', 'B-SMT'), ('-', 'I-SMT'), ('deposited', 'I-SMT'), ('on', 'O'), ('CoTaZr', 'B-MAT'), ('soft', 'B-PRO'), ('-', 'I-PRO'), ('magnetic', 'I-PRO'), ('underlayers', 'I-PRO'), ('and', 'O'), ('the', 'O'), ('effects', 'O'), ('of', 'O'), ('a', 'O'), ('carbon', 'B-MAT'), ('intermediate', 'O'), ('layer', 'B-DSC'), ('on', 'O'), ('magnetic', 'B-PRO'), ('and', 'O'), ('recording', 'B-PRO'), ('properties', 'I-PRO'), ('were', 'O'), ('investigated', 'O')]] BiLSTM-NER - Simpler model \u00b6 matscholar_2020v1 - ner_simple \u00b6 There is an alternative model for NER which simplifies the annotation scheme to not include PVL or PUT and which merges B / I multitoken entities into contiguous strings. To use it, use the ner_simple model from matscholar_2020v1 : from lbnlp.models.load.matscholar_2020v1 import load ner_simple = load ( \"ner_simple\" ) This model has the same tag_doc interface as the original ner model. doc = \"CoCrPt/CoCr/carbon films were sputter-deposited on CoTaZr soft-magnetic\" \"underlayers and the effects of a carbon intermediate layer on magnetic and \" \"recording properties were investigated\" tags = ner_simple . tag_doc ( doc ) print ( tags ) The output joins the IOB scheme into only the 7 entity types. The rest of the format is identical to the original ner model: [[('CoCrPt', 'MAT'), ('/', 'O'), ('CoCr', 'MAT'), ('/', 'O'), ('carbon', 'MAT'), ('films', 'DSC'), ('were', 'O'), ('sputter - deposited', 'SMT'), ('on', 'O'), ('CoTaZr', 'MAT'), ('soft - magneticunderlayers', 'PRO'), ('and', 'O'), ('the', 'O'), ('effects', 'O'), ('of', 'O'), ('a', 'O'), ('carbon', 'MAT'), ('intermediate', 'O'), ('layer', 'DSC'), ('on', 'O'), ('magnetic', 'PRO'), ('and', 'O'), ('recording properties', 'PRO'), ('were', 'O'), ('investigated', 'O')]] MatBERT-NER for Solid State, Gold Nanoparticle, and Dopant data \u00b6 matbert_ner_2021v1 - solid_state , aunp2 , aunp11 , and doping \u00b6 Let's load a more involved MatBERT model and use it for an example: from lbnlp.models.load.matbert_ner_2021v1 import load bert_ner = load ( \"solid_state\" ) Let's see if we can tag an excerpt from a materials science abstract. The MatBERT-NER model works with lists of texts as input, returning lists of summary documents as output. doc = \"Synthesis of carbon nanotubes by chemical vapor deposition over patterned \" \\ \"catalyst arrays leads to nanotubes grown from specific sites on surfaces.\" \\ \"The growth directions of the nanotubes can be controlled by van der Waals \" \\ \"self-assembly forces and applied electric fields. The patterned growth \" \\ \"approach is feasible with discrete catalytic nanoparticles and scalable \" \\ \"on large wafers for massive arrays of novel nanowires.\" # the MatBERT model is intended to be used with batches (multiple documents at once) # so we just put the doc into a list before tagging tags = bert_ner . tag_docs ([ doc ]) print ( tags ) We obtain a summary document for this abstract based on the extracted entities: [{' entities ' : {' APL ' : [' nanotechnology ', ' catalyst ', ' photochemistry ', ' molecular sensors ', ' catalytic ', ' devices ', ' chemical functionalization ', ' nanoscience '], ' CMT ' : [], ' DSC ' : [' nanotubes ', ' wafers ', ' surfaces ', ' nanowires ', ' nanoparticles '], ' MAT ' : [' carbon '], ' PRO ' : [' electromechanical properties ', ' electrical ', ' mechanical ', ' surface chemistry '], ' SMT ' : [' chemical vapor deposition '], ' SPL ' : []}, ' tokens ' : [[{' annotation ' : 'O' , ' text ' : ' synthesis '}, {' annotation ' : 'O' , ' text ' : ' of '}, {' annotation ' : ' MAT ', ' text ' : ' carbon '}, {' annotation ' : ' DSC ', ' text ' : ' nanotubes '}, {' annotation ' : 'O' , ' text ' : ' by '}, {' annotation ' : ' SMT ', ' text ' : ' chemical '}, {' annotation ' : ' SMT ', ' text ' : ' vapor '}, {' annotation ' : ' SMT ', ' text ' : ' deposition '}, {' annotation ' : 'O' , ' text ' : ' over '}, {' annotation ' : 'O' , ' text ' : ' patterned '}, {' annotation ' : ' APL ', ' text ' : ' catalyst '}, {' annotation ' : 'O' , ' text ' : ' arrays '}, {' annotation ' : 'O' , ' text ' : ' leads '}, {' annotation ' : 'O' , ' text ' : ' to '}, {' annotation ' : ' DSC ', ' text ' : ' nanotubes '}, {' annotation ' : 'O' , ' text ' : ' grown '}, {' annotation ' : 'O' , ' text ' : ' from '}, {' annotation ' : 'O' , ' text ' : ' specific '}, {' annotation ' : 'O' , ' text ' : ' sites '}, {' annotation ' : 'O' , ' text ' : ' on '}, {' annotation ' : ' DSC ', ' text ' : ' surfaces '}, {' annotation ' : 'O' , ' text ' : '.' }], [{' annotation ' : 'O' , ' text ' : ' the '}, {' annotation ' : 'O' , ' text ' : ' growth '}, {' annotation ' : 'O' , ' text ' : ' directions '}, {' annotation ' : 'O' , ' text ' : ' of '}, {' annotation ' : 'O' , ' text ' : ' the '}, {' annotation ' : ' DSC ', ' text ' : ' nanotubes '}, {' annotation ' : 'O' , ' text ' : ' can '}, {' annotation ' : 'O' , ' text ' : ' be '}, {' annotation ' : 'O' , ' text ' : ' controlled '}, {' annotation ' : 'O' , ' text ' : ' by '}, {' annotation ' : 'O' , ' text ' : ' van '}, {' annotation ' : 'O' , ' text ' : ' der '}, {' annotation ' : 'O' , ' text ' : ' waals '}, {' annotation ' : 'O' , ' text ' : ' self '}, {' annotation ' : 'O' , ' text ' : '-' }, {' annotation ' : 'O' , ' text ' : ' assembly '}, {' annotation ' : 'O' , ' text ' : ' forces '}, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : 'O' , ' text ' : ' applied '}, {' annotation ' : 'O' , ' text ' : ' electric '}, {' annotation ' : 'O' , ' text ' : ' fields '}, {' annotation ' : 'O' , ' text ' : '.' }], [{' annotation ' : 'O' , ' text ' : ' the '}, {' annotation ' : 'O' , ' text ' : ' patterned '}, {' annotation ' : 'O' , ' text ' : ' growth '}, {' annotation ' : 'O' , ' text ' : ' approach '}, {' annotation ' : 'O' , ' text ' : ' is '}, {' annotation ' : 'O' , ' text ' : ' feasible '}, {' annotation ' : 'O' , ' text ' : ' with '}, {' annotation ' : 'O' , ' text ' : ' discrete '}, {' annotation ' : ' APL ', ' text ' : ' catalytic '}, {' annotation ' : ' DSC ', ' text ' : ' nanoparticles '}, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : 'O' , ' text ' : ' scalable '}, {' annotation ' : 'O' , ' text ' : ' on '}, {' annotation ' : 'O' , ' text ' : ' large '}, {' annotation ' : ' DSC ', ' text ' : ' wafers '}, {' annotation ' : 'O' , ' text ' : ' for '}, {' annotation ' : 'O' , ' text ' : ' massive '}, {' annotation ' : 'O' , ' text ' : ' arrays '}, {' annotation ' : 'O' , ' text ' : ' of '}, {' annotation ' : 'O' , ' text ' : ' novel '}, {' annotation ' : ' DSC ', ' text ' : ' nanowires '}, {' annotation ' : 'O' , ' text ' : '.' }], [{' annotation ' : 'O' , ' text ' : ' controlled '}, {' annotation ' : 'O' , ' text ' : ' synthesis '}, {' annotation ' : 'O' , ' text ' : ' of '}, {' annotation ' : ' DSC ', ' text ' : ' nanotubes '}, {' annotation ' : 'O' , ' text ' : ' opens '}, {' annotation ' : 'O' , ' text ' : ' up '}, {' annotation ' : 'O' , ' text ' : ' exciting '}, {' annotation ' : 'O' , ' text ' : ' opportunities '}, {' annotation ' : 'O' , ' text ' : ' in '}, {' annotation ' : ' APL ', ' text ' : ' nanoscience '}, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : ' APL ', ' text ' : ' nanotechnology '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : 'O' , ' text ' : ' including '}, {' annotation ' : ' PRO ', ' text ' : ' electrical '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : ' PRO ', ' text ' : ' mechanical '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : ' PRO ', ' text ' : ' electromechanical '}, {' annotation ' : ' PRO ', ' text ' : ' properties '}, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : ' APL ', ' text ' : ' devices '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : ' APL ', ' text ' : ' chemical '}, {' annotation ' : ' APL ', ' text ' : ' functionalization '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : ' PRO ', ' text ' : ' surface '}, {' annotation ' : ' PRO ', ' text ' : ' chemistry '}, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : ' APL ', ' text ' : ' photochemistry '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : ' APL ', ' text ' : ' molecular '}, {' annotation ' : ' APL ', ' text ' : ' sensors '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : 'O' , ' text ' : ' interfacing '}, {' annotation ' : 'O' , ' text ' : ' with '}, {' annotation ' : 'O' , ' text ' : ' soft '}, {' annotation ' : 'O' , ' text ' : ' biological '}, {' annotation ' : 'O' , ' text ' : ' systems '}, {' annotation ' : 'O' , ' text ' : '.' }]]}] The entities key represents a summary of each entity found in the document. The tokens key contains a dictionary of each token and its associated predicted label, separated into sentences by lists. The other models such as doping (3-tag scheme), aunp2 (2-tag scheme gold nanoparticle), and aunp11 (11-tag scheme for gold nanoparticle) will have more explanation in an upcoming publication.","title":"Models - Named Entity Recognition"},{"location":"pretrained/02ner/#models-named-entity-recognition","text":"","title":"Models - Named Entity Recognition"},{"location":"pretrained/02ner/#bilstm-ner-for-solid-state-materials-data","text":"","title":"BiLSTM-NER for Solid State Materials Data"},{"location":"pretrained/02ner/#matscholar_2020v1-ner","text":"The BiLSTM-NER model tags inorganic solid-state entities in materials science. Read more details in the original publication. We can import this model from the matscholar_2020v1 model package. Note: this model requires a specific, older version (1.15.0) of tensorflow in order to run. # Import the load function from the model package from lbnlp.models.load.matscholar_2020v1 import load ner_model = load ( \"ner\" ) Our NER model is now loaded. Let's annotate a document. doc = \"CoCrPt/CoCr/carbon films were sputter-deposited on CoTaZr soft-magnetic\" \"underlayers and the effects of a carbon intermediate layer on magnetic and \" \"recording properties were investigated\" tags = ner_model . tag_doc ( doc ) print ( tags ) The tokens in the doc are annotated as tuples according to the IOB (inside entity I , outside entity O , beginning of entity B ) scheme to handle multi-token entities and the following entity tags MAT : material DSC : description of sample SPL : symmetry or phase label SMT : synthesis method CMT : characterization method PRO : property - may also include PVL (property value) or PUT (property unit) APL : application An example tag would be B-MAT meaning beginning of a material entity followed by I-MAT meaning a continuation of that material entity. The output of our code is a list of sentences - each sentence is a list of tuples coupling the text with the label: [[('CoCrPt', 'B-MAT'), ('/', 'O'), ('CoCr', 'B-MAT'), ('/', 'O'), ('carbon', 'B-MAT'), ('films', 'B-DSC'), ('were', 'O'), ('sputter', 'B-SMT'), ('-', 'I-SMT'), ('deposited', 'I-SMT'), ('on', 'O'), ('CoTaZr', 'B-MAT'), ('soft', 'B-PRO'), ('-', 'I-PRO'), ('magnetic', 'I-PRO'), ('underlayers', 'I-PRO'), ('and', 'O'), ('the', 'O'), ('effects', 'O'), ('of', 'O'), ('a', 'O'), ('carbon', 'B-MAT'), ('intermediate', 'O'), ('layer', 'B-DSC'), ('on', 'O'), ('magnetic', 'B-PRO'), ('and', 'O'), ('recording', 'B-PRO'), ('properties', 'I-PRO'), ('were', 'O'), ('investigated', 'O')]]","title":"matscholar_2020v1 - ner"},{"location":"pretrained/02ner/#bilstm-ner-simpler-model","text":"","title":"BiLSTM-NER - Simpler model"},{"location":"pretrained/02ner/#matscholar_2020v1-ner_simple","text":"There is an alternative model for NER which simplifies the annotation scheme to not include PVL or PUT and which merges B / I multitoken entities into contiguous strings. To use it, use the ner_simple model from matscholar_2020v1 : from lbnlp.models.load.matscholar_2020v1 import load ner_simple = load ( \"ner_simple\" ) This model has the same tag_doc interface as the original ner model. doc = \"CoCrPt/CoCr/carbon films were sputter-deposited on CoTaZr soft-magnetic\" \"underlayers and the effects of a carbon intermediate layer on magnetic and \" \"recording properties were investigated\" tags = ner_simple . tag_doc ( doc ) print ( tags ) The output joins the IOB scheme into only the 7 entity types. The rest of the format is identical to the original ner model: [[('CoCrPt', 'MAT'), ('/', 'O'), ('CoCr', 'MAT'), ('/', 'O'), ('carbon', 'MAT'), ('films', 'DSC'), ('were', 'O'), ('sputter - deposited', 'SMT'), ('on', 'O'), ('CoTaZr', 'MAT'), ('soft - magneticunderlayers', 'PRO'), ('and', 'O'), ('the', 'O'), ('effects', 'O'), ('of', 'O'), ('a', 'O'), ('carbon', 'MAT'), ('intermediate', 'O'), ('layer', 'DSC'), ('on', 'O'), ('magnetic', 'PRO'), ('and', 'O'), ('recording properties', 'PRO'), ('were', 'O'), ('investigated', 'O')]]","title":"matscholar_2020v1 - ner_simple"},{"location":"pretrained/02ner/#matbert-ner-for-solid-state-gold-nanoparticle-and-dopant-data","text":"","title":"MatBERT-NER for Solid State, Gold Nanoparticle, and Dopant data"},{"location":"pretrained/02ner/#matbert_ner_2021v1-solid_state-aunp2-aunp11-and-doping","text":"Let's load a more involved MatBERT model and use it for an example: from lbnlp.models.load.matbert_ner_2021v1 import load bert_ner = load ( \"solid_state\" ) Let's see if we can tag an excerpt from a materials science abstract. The MatBERT-NER model works with lists of texts as input, returning lists of summary documents as output. doc = \"Synthesis of carbon nanotubes by chemical vapor deposition over patterned \" \\ \"catalyst arrays leads to nanotubes grown from specific sites on surfaces.\" \\ \"The growth directions of the nanotubes can be controlled by van der Waals \" \\ \"self-assembly forces and applied electric fields. The patterned growth \" \\ \"approach is feasible with discrete catalytic nanoparticles and scalable \" \\ \"on large wafers for massive arrays of novel nanowires.\" # the MatBERT model is intended to be used with batches (multiple documents at once) # so we just put the doc into a list before tagging tags = bert_ner . tag_docs ([ doc ]) print ( tags ) We obtain a summary document for this abstract based on the extracted entities: [{' entities ' : {' APL ' : [' nanotechnology ', ' catalyst ', ' photochemistry ', ' molecular sensors ', ' catalytic ', ' devices ', ' chemical functionalization ', ' nanoscience '], ' CMT ' : [], ' DSC ' : [' nanotubes ', ' wafers ', ' surfaces ', ' nanowires ', ' nanoparticles '], ' MAT ' : [' carbon '], ' PRO ' : [' electromechanical properties ', ' electrical ', ' mechanical ', ' surface chemistry '], ' SMT ' : [' chemical vapor deposition '], ' SPL ' : []}, ' tokens ' : [[{' annotation ' : 'O' , ' text ' : ' synthesis '}, {' annotation ' : 'O' , ' text ' : ' of '}, {' annotation ' : ' MAT ', ' text ' : ' carbon '}, {' annotation ' : ' DSC ', ' text ' : ' nanotubes '}, {' annotation ' : 'O' , ' text ' : ' by '}, {' annotation ' : ' SMT ', ' text ' : ' chemical '}, {' annotation ' : ' SMT ', ' text ' : ' vapor '}, {' annotation ' : ' SMT ', ' text ' : ' deposition '}, {' annotation ' : 'O' , ' text ' : ' over '}, {' annotation ' : 'O' , ' text ' : ' patterned '}, {' annotation ' : ' APL ', ' text ' : ' catalyst '}, {' annotation ' : 'O' , ' text ' : ' arrays '}, {' annotation ' : 'O' , ' text ' : ' leads '}, {' annotation ' : 'O' , ' text ' : ' to '}, {' annotation ' : ' DSC ', ' text ' : ' nanotubes '}, {' annotation ' : 'O' , ' text ' : ' grown '}, {' annotation ' : 'O' , ' text ' : ' from '}, {' annotation ' : 'O' , ' text ' : ' specific '}, {' annotation ' : 'O' , ' text ' : ' sites '}, {' annotation ' : 'O' , ' text ' : ' on '}, {' annotation ' : ' DSC ', ' text ' : ' surfaces '}, {' annotation ' : 'O' , ' text ' : '.' }], [{' annotation ' : 'O' , ' text ' : ' the '}, {' annotation ' : 'O' , ' text ' : ' growth '}, {' annotation ' : 'O' , ' text ' : ' directions '}, {' annotation ' : 'O' , ' text ' : ' of '}, {' annotation ' : 'O' , ' text ' : ' the '}, {' annotation ' : ' DSC ', ' text ' : ' nanotubes '}, {' annotation ' : 'O' , ' text ' : ' can '}, {' annotation ' : 'O' , ' text ' : ' be '}, {' annotation ' : 'O' , ' text ' : ' controlled '}, {' annotation ' : 'O' , ' text ' : ' by '}, {' annotation ' : 'O' , ' text ' : ' van '}, {' annotation ' : 'O' , ' text ' : ' der '}, {' annotation ' : 'O' , ' text ' : ' waals '}, {' annotation ' : 'O' , ' text ' : ' self '}, {' annotation ' : 'O' , ' text ' : '-' }, {' annotation ' : 'O' , ' text ' : ' assembly '}, {' annotation ' : 'O' , ' text ' : ' forces '}, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : 'O' , ' text ' : ' applied '}, {' annotation ' : 'O' , ' text ' : ' electric '}, {' annotation ' : 'O' , ' text ' : ' fields '}, {' annotation ' : 'O' , ' text ' : '.' }], [{' annotation ' : 'O' , ' text ' : ' the '}, {' annotation ' : 'O' , ' text ' : ' patterned '}, {' annotation ' : 'O' , ' text ' : ' growth '}, {' annotation ' : 'O' , ' text ' : ' approach '}, {' annotation ' : 'O' , ' text ' : ' is '}, {' annotation ' : 'O' , ' text ' : ' feasible '}, {' annotation ' : 'O' , ' text ' : ' with '}, {' annotation ' : 'O' , ' text ' : ' discrete '}, {' annotation ' : ' APL ', ' text ' : ' catalytic '}, {' annotation ' : ' DSC ', ' text ' : ' nanoparticles '}, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : 'O' , ' text ' : ' scalable '}, {' annotation ' : 'O' , ' text ' : ' on '}, {' annotation ' : 'O' , ' text ' : ' large '}, {' annotation ' : ' DSC ', ' text ' : ' wafers '}, {' annotation ' : 'O' , ' text ' : ' for '}, {' annotation ' : 'O' , ' text ' : ' massive '}, {' annotation ' : 'O' , ' text ' : ' arrays '}, {' annotation ' : 'O' , ' text ' : ' of '}, {' annotation ' : 'O' , ' text ' : ' novel '}, {' annotation ' : ' DSC ', ' text ' : ' nanowires '}, {' annotation ' : 'O' , ' text ' : '.' }], [{' annotation ' : 'O' , ' text ' : ' controlled '}, {' annotation ' : 'O' , ' text ' : ' synthesis '}, {' annotation ' : 'O' , ' text ' : ' of '}, {' annotation ' : ' DSC ', ' text ' : ' nanotubes '}, {' annotation ' : 'O' , ' text ' : ' opens '}, {' annotation ' : 'O' , ' text ' : ' up '}, {' annotation ' : 'O' , ' text ' : ' exciting '}, {' annotation ' : 'O' , ' text ' : ' opportunities '}, {' annotation ' : 'O' , ' text ' : ' in '}, {' annotation ' : ' APL ', ' text ' : ' nanoscience '}, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : ' APL ', ' text ' : ' nanotechnology '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : 'O' , ' text ' : ' including '}, {' annotation ' : ' PRO ', ' text ' : ' electrical '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : ' PRO ', ' text ' : ' mechanical '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : ' PRO ', ' text ' : ' electromechanical '}, {' annotation ' : ' PRO ', ' text ' : ' properties '}, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : ' APL ', ' text ' : ' devices '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : ' APL ', ' text ' : ' chemical '}, {' annotation ' : ' APL ', ' text ' : ' functionalization '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : ' PRO ', ' text ' : ' surface '}, {' annotation ' : ' PRO ', ' text ' : ' chemistry '}, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : ' APL ', ' text ' : ' photochemistry '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : ' APL ', ' text ' : ' molecular '}, {' annotation ' : ' APL ', ' text ' : ' sensors '}, {' annotation ' : 'O' , ' text ' : ',' }, {' annotation ' : 'O' , ' text ' : ' and '}, {' annotation ' : 'O' , ' text ' : ' interfacing '}, {' annotation ' : 'O' , ' text ' : ' with '}, {' annotation ' : 'O' , ' text ' : ' soft '}, {' annotation ' : 'O' , ' text ' : ' biological '}, {' annotation ' : 'O' , ' text ' : ' systems '}, {' annotation ' : 'O' , ' text ' : '.' }]]}] The entities key represents a summary of each entity found in the document. The tokens key contains a dictionary of each token and its associated predicted label, separated into sentences by lists. The other models such as doping (3-tag scheme), aunp2 (2-tag scheme gold nanoparticle), and aunp11 (11-tag scheme for gold nanoparticle) will have more explanation in an upcoming publication.","title":"matbert_ner_2021v1 - solid_state, aunp2, aunp11, and doping"},{"location":"pretrained/03relevance/","text":"Models - Relevance Classification \u00b6 Load a model for Relevance Classification \u00b6 Load the relevance classification model from matscholar_2020v1 by calling: from lbnlp.models.load.matscholar_2020v1 import load clf_model = load ( \"relevance\" ) Let's see how it does in determining relevance for inorganic materials science: not_relevant = \"The polymer was used for an OLED. This can also be used for a biosensor.\" relevant = \"The bandgap of ZnO is 33 eV\" relevance = clf_model . classify_many ([ not_relevant , relevant ]) print ( relevance ) It correctly classifies our sentences: [0, 1]","title":"Models - Relevance Classification"},{"location":"pretrained/03relevance/#models-relevance-classification","text":"","title":"Models - Relevance Classification"},{"location":"pretrained/03relevance/#load-a-model-for-relevance-classification","text":"Load the relevance classification model from matscholar_2020v1 by calling: from lbnlp.models.load.matscholar_2020v1 import load clf_model = load ( \"relevance\" ) Let's see how it does in determining relevance for inorganic materials science: not_relevant = \"The polymer was used for an OLED. This can also be used for a biosensor.\" relevant = \"The bandgap of ZnO is 33 eV\" relevance = clf_model . classify_many ([ not_relevant , relevant ]) print ( relevance ) It correctly classifies our sentences: [0, 1]","title":"Load a model for Relevance Classification"},{"location":"pretrained/11addyourown/","text":"Adding your own trained models to LBNLP \u00b6 1. Store your data \u00b6 Using whatever directory structure you want, store your model(s)' data in some loadable format. 2. Upload your data as a file \u00b6 Zip that directory and upload it to a publicly accessible URL. 3. Make an modelpkg entry \u00b6 Make an entry for a new model package in lbnlp/models/modelpkg_metadata.json , including all needed requirements specific to your models, citations, and descriptions. \"my_model_package_2021v2\" : { \"url\" : \"https://url.to.download/your/file\" , \"hash\" : \"The SHA256 hash of your zip file\" , \"models\" : { \"my_first_model\" : { \"requirements\" : [ \"pip-accessible-requirement==1.0.0\" ], \"description\" : \"Model 1, a good description\" , \"citation\" : null }, \"my_second_model\" : { \"requirements\" : [ \"sklearn==0.19.1\" ], \"description\" : \"Model 2, an even better model!\" , \"citation\" : null 4. Add a data loader \u00b6 Create a new module in lbnlp.models.load with the same name you gave your package in the metadata. This module is where you will put the functions to load your models. Follow the example below to make full use of the helper functions and classes: from lbnlp.models.fetch import ModelPkgLoader from lbnlp.models.util import model_loader_setup # Provides an easy way to access file paths and metadata for your package pkg = ModelPkgLoader ( \"my_model_package_2021v2\" ) # Load is the required name for the loading function. # Keep this function signature the same as shown here # model_loader_setup provides checking and validation in the background @model_loader_setup ( pkg ) def load ( model_name , ignore_requirements = False ): # This is the root of the directory of the zip file you uploaded. base_path = pkg . structured_path # Your code for loading your model goes here The model loader code should only access classes and methods in lbnlp or in the requirements. Make it easy for people to load and use your model! That's it! Congrats on adding your model to LBNLP.","title":"Adding your own trained models to LBNLP"},{"location":"pretrained/11addyourown/#adding-your-own-trained-models-to-lbnlp","text":"","title":"Adding your own trained models to LBNLP"},{"location":"pretrained/11addyourown/#1-store-your-data","text":"Using whatever directory structure you want, store your model(s)' data in some loadable format.","title":"1. Store your data"},{"location":"pretrained/11addyourown/#2-upload-your-data-as-a-file","text":"Zip that directory and upload it to a publicly accessible URL.","title":"2. Upload your data as a file"},{"location":"pretrained/11addyourown/#3-make-an-modelpkg-entry","text":"Make an entry for a new model package in lbnlp/models/modelpkg_metadata.json , including all needed requirements specific to your models, citations, and descriptions. \"my_model_package_2021v2\" : { \"url\" : \"https://url.to.download/your/file\" , \"hash\" : \"The SHA256 hash of your zip file\" , \"models\" : { \"my_first_model\" : { \"requirements\" : [ \"pip-accessible-requirement==1.0.0\" ], \"description\" : \"Model 1, a good description\" , \"citation\" : null }, \"my_second_model\" : { \"requirements\" : [ \"sklearn==0.19.1\" ], \"description\" : \"Model 2, an even better model!\" , \"citation\" : null","title":"3. Make an modelpkg entry"},{"location":"pretrained/11addyourown/#4-add-a-data-loader","text":"Create a new module in lbnlp.models.load with the same name you gave your package in the metadata. This module is where you will put the functions to load your models. Follow the example below to make full use of the helper functions and classes: from lbnlp.models.fetch import ModelPkgLoader from lbnlp.models.util import model_loader_setup # Provides an easy way to access file paths and metadata for your package pkg = ModelPkgLoader ( \"my_model_package_2021v2\" ) # Load is the required name for the loading function. # Keep this function signature the same as shown here # model_loader_setup provides checking and validation in the background @model_loader_setup ( pkg ) def load ( model_name , ignore_requirements = False ): # This is the root of the directory of the zip file you uploaded. base_path = pkg . structured_path # Your code for loading your model goes here The model loader code should only access classes and methods in lbnlp or in the requirements. Make it easy for people to load and use your model! That's it! Congrats on adding your model to LBNLP.","title":"4. Add a data loader"}]}